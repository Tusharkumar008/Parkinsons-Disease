# -*- coding: utf-8 -*-
"""PARKINSINS DISEASE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fp9hQD6yFd7HBikp2TFXkYCc3-ed_8kF

## PARKINSON'S DISEASE DETECTION
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

"""LOADING THE DATA FROM CSV FILE TO PANDAS DATA FILE"""

pk_data = pd.read_csv('/content/parkinsons.csv')

# printing the first 5 row of the dataset(we use head command whixh prints first five rows of the datset)
pk_data.head()

# NO OF ROW AND COLUMNS IN THE DATASET(we use shape keyword to identify the number of rows and columns in the dataset)
pk_data.shape

# MORE INFORMATIONS ABOUT THE DATASET
pk_data.info()

pk_data.drop('name', inplace= True, axis = 1)

# TO CHECK THE MISSING VALUES(isnull().sum()show the missing value is present or not)
pk_data.isnull().sum()

# Getting some statical measure about the data (we use describe() keyword )
 pk_data.describe()

# Distribution of target variable
pk_data['status'].value_counts()

"""0 -- > Healthy

1 -- > parkinson's +ive
"""

print(pk_data.dtypes)

# Grouping the data based on target variable and calculating mean for numeric columns only
pk_data.groupby('status').mean(numeric_only=True)

def is_string(x):
    return isinstance(x, str)

# Apply the is_string function to each element
string_data = pk_data.applymap(is_string)

# Get the positions of string values
string_positions = np.where(string_data)

# Print the locations and the string values
for row, col in zip(*string_positions):
    print(f"String value at row {row}, column '{pk_data.columns[col]}': {pk_data.iat[row, col]}")

# Grouping the data based on target variable
pk_data.groupby('status').mean()

"""DATA PREPROCESSING"""

#seprating feature and target
x = pk_data.drop(columns=['status'], axis=1)
y = pk_data['status']

print(x)

print(y)

# Spliting the data to traning data and test data
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)

print(x.shape,x_train.shape,x_test.shape)

"""Data Standardizationn"""

scaler = StandardScaler()

scaler.fit(x_train)

x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

print(x_train)

# model traning
#Support Vector Machine Model

model = svm.SVC(kernel='linear')

# traning the svm model with traning data
model.fit(x_train,y_train)

"""Model Evaluation"""

# Accuracy score on traning data
 x_train_prediction= model.predict(x_train)
 training_data_accuracy= accuracy_score(y_train,x_train_prediction)

print('accuracy score of traning data:',training_data_accuracy)

# Accuract score of test data
x_test_prediction= model.predict(x_test)
test_data_accuracy=accuracy_score(y_test,x_test_prediction)

print('accuracy score of test data:',test_data_accuracy)

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

knn = KNeighborsClassifier()
knn.fit(x_train, y_train)

knn_acc = accuracy_score(y_test, knn.predict(x_test))
print(f"Training Accuracy of KNN is {accuracy_score(y_train, knn.predict(x_train))}")
print(f"Testing Accuracy of KNN is {accuracy_score(y_test, knn.predict(x_test))}")

print(f"Confusion Matrix of KNN is \n {confusion_matrix(y_test, knn.predict(x_test))}\n")
print(f"Classification Report of KNN is \n{classification_report(y_test, knn.predict(x_test))}")

"""Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)

dtc_acc = accuracy_score(y_test, dtc.predict(x_test))
print(f"Training Accuracy of DTC is {accuracy_score(y_train, dtc.predict(x_train))}")
print(f"Testing Accuracy of DTC is {accuracy_score(y_test, dtc.predict(x_test))}")

print(f"Confusion Matrix of DTC is \n {confusion_matrix(y_test, dtc.predict(x_test))}\n")
print(f"Classification Report of DTC is \n{classification_report(y_test, dtc.predict(x_test))}")

"""Hyper Parameter Tuning"""

from sklearn.model_selection import GridSearchCV

GRID_PARAMETER = {
    'criterion':['gini','entropy'],
    'max_depth':[3,5,7,10],
    'splitter':['best','random'],
    'min_samples_leaf':[1,2,3,5,7],
    'min_samples_split':[1,2,3,5,7],
    'max_features':['auto', 'sqrt', 'log2']
}

grid_search_dtc = GridSearchCV(dtc, GRID_PARAMETER, cv=5, n_jobs=-1, verbose = 1)
grid_search_dtc.fit(x_train, y_train)

# best paramer and best score
print(grid_search_dtc.best_params_)
print(grid_search_dtc.best_score_)

dtc = grid_search_dtc.best_estimator_

dtc_acc = accuracy_score(y_test, dtc.predict(x_test))
print(f"Training Accuracy of DTC is {accuracy_score(y_train, dtc.predict(x_train))}")
print(f"Testing Accuracy of DTC is {accuracy_score(y_test, dtc.predict(x_test))}")

print(f"Confusion Matrix of DTC is \n {confusion_matrix(y_test, dtc.predict(x_test))}\n")
print(f"Classification Report of DTC is \n{classification_report(y_test, dtc.predict(x_test))}")

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(x_train, y_train)

lr_acc = accuracy_score(y_test, lr.predict(x_test))
print(f"Training Accuracy of LR is {accuracy_score(y_train, lr.predict(x_train))}")
print(f"Testing Accuracy of LR is {accuracy_score(y_test, lr.predict(x_test))}")

print(f"Confusion Matrix of LR is \n {confusion_matrix(y_test, lr.predict(x_test))}\n")
print(f"Classification Report of LR is \n{classification_report(y_test, lr.predict(x_test))}")

"""Model Comparision"""

models = pd.DataFrame({
    'Model':['Logistic Regression', 'KNN', ' dtc'],
    'Score':[lr_acc, knn_acc, dtc_acc]
})

models.sort_values(by='Score', ascending = False)

from sklearn import metrics
plt.figure(figsize=(8,5))
models = [
{
    'label': 'LR',
    'model': lr,
},
{
    'label': 'KNN',
    'model': knn,
},
{
    'label': 'DTC',
    'model': dtc,
}
]
for m in models:
    model = m['model']
    model.fit(x_train, y_train)
    y_pred=model.predict(x_test)
    fpr1, tpr1, thresholds = metrics.roc_curve(y_test, model.predict_proba(x_test)[:,1])
    auc = metrics.roc_auc_score(y_test,model.predict(x_test))
    plt.plot(fpr1, tpr1, label='%s - ROC (area = %0.2f)' % (m['label'], auc))

plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=12)
plt.ylabel('Sensitivity (True Positive Rate)', fontsize=12)
plt.title('ROC - Kidney Disease Prediction', fontsize=12)
plt.legend(loc="lower right", fontsize=12)
plt.savefig("roc_kidney.jpeg", format='jpeg', dpi=400, bbox_inches='tight')
plt.show()

input_data = (119.99200,157.30200,74.99700,0.00784,0.00007,0.00370,0.00554,0.01109,0.04374,0.42600,0.02182,0.03130,0.02971,0.06545,0.02211,21.03300,0.414783,0.815285,-4.813031,0.266482,2.301442,0.284654)

# changing input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the numpy array
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

#standardize the data
std_data = scaler.transform(input_data_reshaped)

prediction = model.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print("The person does not have parkinson's disease")
else:
  print("The person has parkinson's disease")

# Function to take user input
def get_user_input():
    print("Please enter the following values for prediction:")
    MDVP_Fo = float(input("MDVP:Fo(Hz) - Average vocal fundamental frequency: "))
    MDVP_Fhi = float(input("MDVP:Fhi(Hz) - Maximum vocal fundamental frequency: "))
    MDVP_Flo = float(input("MDVP:Flo(Hz) - Minimum vocal fundamental frequency: "))
    MDVP_Jitter_percent = float(input("MDVP:Jitter(%) - Jitter (local): "))
    MDVP_Jitter_Abs = float(input("MDVP:Jitter(Abs) - Jitter (local, absolute): "))
    MDVP_RAP = float(input("MDVP:RAP - Jitter (rap): "))
    MDVP_PPQ = float(input("MDVP:PPQ - Jitter (ppq5): "))
    Jitter_DDP = float(input("Jitter:DDP - Jitter (ddp): "))
    MDVP_Shimmer = float(input("MDVP:Shimmer - Shimmer (local): "))
    MDVP_Shimmer_dB = float(input("MDVP:Shimmer(dB) - Shimmer (local, dB): "))
    Shimmer_APQ3 = float(input("Shimmer:APQ3 - Shimmer (apq3): "))
    Shimmer_APQ5 = float(input("Shimmer:APQ5 - Shimmer (apq5): "))
    MDVP_APQ = float(input("MDVP:APQ - Shimmer (apq): "))
    Shimmer_DDA = float(input("Shimmer:DDA - Shimmer (dda): "))
    NHR = float(input("NHR - Noise-to-harmonics ratio: "))
    HNR = float(input("HNR - Harmonics-to-noise ratio: "))
    RPDE = float(input("RPDE - Recurrence period density entropy: "))
    DFA = float(input("DFA - Detrended fluctuation analysis: "))
    spread1 = float(input("spread1 - Nonlinear measure of fundamental frequency variation: "))
    spread2 = float(input("spread2 - Nonlinear measure of fundamental frequency variation: "))
    D2 = float(input("D2 - Nonlinear measure of signal fractal scaling exponent: "))
    PPE = float(input("PPE - Nonlinear measure of signal fundamental frequency: "))

    return (MDVP_Fo, MDVP_Fhi, MDVP_Flo, MDVP_Jitter_percent, MDVP_Jitter_Abs, MDVP_RAP,
            MDVP_PPQ, Jitter_DDP, MDVP_Shimmer, MDVP_Shimmer_dB, Shimmer_APQ3, Shimmer_APQ5,
            MDVP_APQ, Shimmer_DDA, NHR, HNR, RPDE, DFA, spread1, spread2, D2, PPE)

# Get user input
input_data = get_user_input()

# Changing input data to a numpy array
input_data_as_numpy_array = np.asarray(input_data)

# Reshape the numpy array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1, -1)

# Standardize the data
std_data = scaler.transform(input_data_reshaped)

# Make a prediction
prediction = model.predict(std_data)

# Display the result
if prediction[0] == 0:
    print("The person does not have Parkinson's disease.")
else:
    print("The person has Parkinson's disease.")